{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5c7e7ed6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from typing import List\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a0664fdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) Dotted abbreviations like U.S.A., U.K., Ph.D.\n",
    "ABBREV_DOTTED = r\"(?:[A-Za-z]\\.){2,}[A-Za-z]?\"  # e.g., U.S.A. or U.S.A\n",
    "\n",
    "# 2) All-caps acronyms (2+ letters), keep as one token (USA, NATO)\n",
    "ACRONYM_ALLCAPS = r\"(?:[A-Z]{2,})\"\n",
    "\n",
    "# 3) Internal hyphenations (ice-cream, mother-in-law)\n",
    "HYPHENATED = r\"(?:[A-Za-z]+(?:-[A-Za-z]+)+)\"\n",
    "\n",
    "# 4) Words (letters only), we’ll split contractions in post-processing\n",
    "WORD = r\"(?:[A-Za-z]+)\"\n",
    "\n",
    "# 5) Numbers (with optional commas/decimals) – kept as one token\n",
    "NUMBER = r\"(?:\\d{1,3}(?:,\\d{3})+(?:\\.\\d+)?|\\d+\\.\\d+|\\d+)\"\n",
    "\n",
    "# 6) Any single punctuation/special symbol (one char)\n",
    "PUNCT_OR_SYMBOL = r\"(?:[^\\w\\s])\"  # anything not a letter/digit/_ or whitespace\n",
    "\n",
    "# Master pattern: try the most specific first\n",
    "TOKEN_PATTERN = re.compile(\n",
    "    \"|\".join([\n",
    "        ABBREV_DOTTED,\n",
    "        ACRONYM_ALLCAPS,\n",
    "        HYPHENATED,\n",
    "        NUMBER,\n",
    "        WORD,\n",
    "        PUNCT_OR_SYMBOL\n",
    "    ])\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "521e1291",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Common apostrophe contractions to split if present: it's -> it + 's (we'll output: it, s)\n",
    "APOSTROPHE_CONTRACTIONS = [\n",
    "    r\"(.*?)(n)['’]?t$\",   # don't -> do + n't  (we'll output as do, nt)\n",
    "    r\"(.*?)[‘’']re$\",     # you're -> you + re\n",
    "    r\"(.*?)[‘’']ve$\",     # they've -> they + ve\n",
    "    r\"(.*?)[‘’']ll$\",     # I'll -> I + ll\n",
    "    r\"(.*?)[‘’']d$\",      # I'd -> I + d  (would/had)\n",
    "    r\"(.*?)[‘’']m$\",      # I'm -> I + m\n",
    "    r\"(.*?)[‘’']s$\",      # it's -> it + s  (is/has or possessive; we keep it uniform)\n",
    "]\n",
    "APOSTROPHE_CONTRACTIONS = [re.compile(p, re.IGNORECASE) for p in APOSTROPHE_CONTRACTIONS]\n",
    "\n",
    "# No-apostrophe nt-case (e.g., isnt -> is + nt, dont -> do + nt)\n",
    "# Restrict to typical bases to avoid false splits like \"paint\"\n",
    "NT_BASES = {\n",
    "    \"is\",\"are\",\"am\",\"do\",\"does\",\"did\",\"have\",\"has\",\"had\",\n",
    "    \"can\",\"could\",\"will\",\"would\",\"should\",\"shall\",\"must\",\n",
    "    \"might\",\"may\",\"was\",\"were\",\"ain\",\"won\",\"don\"\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "89a7bc9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_contraction(token: str) -> List[str]:\n",
    "    \"\"\"\n",
    "    Split contractions into two tokens.\n",
    "    - Apostrophe forms: you're -> you + re; isn't -> is + nt\n",
    "    - No-apostrophe 'nt' forms: isnt -> is + nt (for known bases)\n",
    "    Returns [token] unchanged if no split applies.\n",
    "    \"\"\"\n",
    "    t = token\n",
    "\n",
    "    # 4a) Apostrophe-based splits\n",
    "    for pat in APOSTROPHE_CONTRACTIONS:\n",
    "        m = pat.match(t)\n",
    "        if m:\n",
    "            root = m.group(1)\n",
    "            # Special case for n't: pattern captures n separately\n",
    "            if len(m.groups()) >= 2 and m.group(2) == 'n':\n",
    "                return [root, \"nt\"]\n",
    "            # For other endings, take trailing part from the regex pattern itself\n",
    "            suffix = pat.pattern.split(\"]\")[-1]  # crude, we'll compute directly instead\n",
    "            # Better: derive suffix by removing the root\n",
    "            # Redo simply: check explicit endings\n",
    "            lower = t.lower()\n",
    "            for end in (\"'re\",\"’re\",\"re\",\"'ve\",\"’ve\",\"ve\",\"'ll\",\"’ll\",\"ll\",\"'d\",\"’d\",\"d\",\"'m\",\"’m\",\"m\",\"'s\",\"’s\",\"s\"):\n",
    "                if lower.endswith(end):\n",
    "                    return [root, end.replace(\"'\", \"\").replace(\"’\",\"\")]\n",
    "            # Fallback: no split\n",
    "            return [token]\n",
    "\n",
    "    # 4b) No-apostrophe \"nt\" (isnt, dont, cant)\n",
    "    lower = t.lower()\n",
    "    if lower.endswith(\"nt\"):\n",
    "        stem = lower[:-2]\n",
    "        if stem in NT_BASES:\n",
    "            # Respect original casing for the stem if possible\n",
    "            return [t[:-2], \"nt\"]\n",
    "\n",
    "    return [token]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "396ec85e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text: str) -> List[str]:\n",
    "    \"\"\"\n",
    "    1) Use regex to extract preliminary tokens.\n",
    "    2) Post-process to split contractions.\n",
    "    3) Keep punctuation/symbols as standalone tokens.\n",
    "    4) Keep dotted abbreviations, acronyms, and hyphenations as single tokens.\n",
    "    \"\"\"\n",
    "    raw_tokens = [m.group(0) for m in TOKEN_PATTERN.finditer(text)]\n",
    "\n",
    "    tokens: List[str] = []\n",
    "    for tok in raw_tokens:\n",
    "        # If it's a pure word, attempt contraction splitting.\n",
    "        # Words = letters only (matches our WORD). Others we leave alone.\n",
    "        if re.fullmatch(WORD, tok):\n",
    "            tokens.extend(split_contraction(tok))\n",
    "        else:\n",
    "            tokens.append(tok)\n",
    "\n",
    "    return tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "010405c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['He', 'said', ':', 'U.S.A.', 'and', 'USA', 'are', 'both', 'abbreviations', '.', 'I', 'love', 'ice-cream', '!', 'But', 'he', 'does', 'nt', ';', 'she', 'isn', \"'\", 't', ';', 'you', '’', 're', ';', 'I', \"'\", 'd', ';', 'it', '’', 's', 'fine', '.', 'Numbers', 'like', '1,234.56', 'and', '42', 'are', 'tokens', '.', 'Contraction', 'without', 'apostrophe', ':', 'is', 'nt', ',', 'do', 'nt', ',', 'ca', 'nt', '.']\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    sample = (\n",
    "        \"He said: U.S.A. and USA are both abbreviations. \"\n",
    "        \"I love ice-cream! But he doesnt; she isn't; you’re; I'd; it’s fine. \"\n",
    "        \"Numbers like 1,234.56 and 42 are tokens. \"\n",
    "        \"Contraction without apostrophe: isnt, dont, cant.\"\n",
    "    )\n",
    "    print(tokenize(sample))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vgg16_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
